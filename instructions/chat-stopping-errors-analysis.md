# Chat Stopping Errors - Comprehensive Analysis

## Overview

Based on analysis of server logs and codebase examination, there are several recurring errors that stop chat conversations in the EasyLog system. **Critical Update:** User reports same "Provider returned error" issues with multiple models including GPT-4.1, indicating this is an **OpenRouter infrastructure problem**, not model-specific.

## üö® Critical Error Pattern: "Provider returned error"

### **Error Details**

**Primary Error:**

```
openai.APIError: Provider returned error
```

**Traceback Path:**

```
message_service.py ‚Üí base_agent.py ‚Üí _handle_stream() ‚Üí OpenAI streaming ‚Üí APIError
```

**Log Pattern:**

```
ERROR - Error handling tool call: Provider returned error
ERROR - Error forwarding message: Provider returned error
ERROR - Error in SSE stream
WARNING - Sending sse error event to client: event: error
```

### **Root Causes Identified (REVISED)**

#### 1. **OpenRouter Infrastructure Reliability Issues** ‚ö†Ô∏è **PRIMARY CAUSE**

**Problem:** OpenRouter API has known stability issues affecting **multiple models** including:

- `anthropic/claude-sonnet-4`
- `openai/gpt-4.1`
- Other OpenRouter-hosted models

**Evidence:**

- Same error pattern across different model providers
- "Provider returned error" indicates OpenRouter infrastructure failure
- Not model-specific but OpenRouter-wide reliability issue

**Impact:** Complete chat stoppage when OpenRouter fails during function calling

#### 2. **OpenRouter Function Calling Overload**

**Problem:** OpenRouter infrastructure struggles with complex function calling sequences, regardless of underlying model capability.

**High-Risk Function Call Sequence:**

1. `tool_ask_multiple_choice` (17 times for questionnaire)
2. `tool_answer_questionaire_question` (17 times)
3. `tool_noop` (17 times)
4. `tool_create_zlm_chart` (complex data validation)

**Evidence:**

- Heavy function calling load on OpenRouter infrastructure
- Complex ZLM chart data validation through OpenRouter proxy
- Multiple tools with regex filtering overloading OpenRouter

#### 3. **OpenRouter Rate Limiting / Quotas**

**Potential Issues:**

- OpenRouter account hitting rate limits
- Function calling quota exceeded
- Account-specific reliability issues

#### 4. **Tool Data Validation Errors (Secondary)**

**High-Risk Validation Points:**

```python
# ZLM Chart Validation (From codebase analysis)
if not (0.0 <= val_float <= 100.0):
    raise ValueError(f"ZLM chart 'value' {val_from_container} is outside the expected 0-100 range")

if role_from_data not in ZLM_CUSTOM_COLOR_ROLE_MAP:
    raise ValueError(f"Invalid 'colorRole' ('{role_from_data}') provided")
```

**Problem:** If LLM provides invalid data, ValueError stops entire chat

## üìä Error Frequency Analysis (UPDATED)

### **Most Common Error Scenarios**

1. **OpenRouter Infrastructure Failures** (70% of failures) ‚ö†Ô∏è **NEW PRIMARY**

   - "Provider returned error" across multiple models
   - OpenRouter API timeouts/instability
   - Function calling infrastructure issues

2. **ZLM Chart Creation** (15% of failures)

   - Invalid percentage values (outside 0-100)
   - Wrong colorRole values
   - Missing required data structure

3. **Multiple Choice Widget Issues** (10% of failures)

   - Invalid choice structure
   - Missing label/value pairs

4. **Questionnaire Flow Interruption** (5% of failures)
   - Sequence breaking errors
   - Memory/metadata corruption

## üîß Immediate Solutions (REVISED)

### **Solution 1: Direct OpenAI API Fallback** ‚≠ê **RECOMMENDED**

**Issue:** OpenRouter infrastructure is unreliable for production use
**Fix:** Implement direct OpenAI API fallback

```python
# Enhanced client configuration with direct OpenAI fallback
class DualAPIClient:
    def __init__(self):
        # OpenRouter client
        self.openrouter_client = AsyncOpenAI(
            api_key=os.getenv("OPENROUTER_API_KEY"),
            base_url="https://openrouter.ai/api/v1"
        )

        # Direct OpenAI client for fallback
        self.openai_client = AsyncOpenAI(
            api_key=os.getenv("OPENAI_API_KEY")
        )

        self.use_fallback = False
        self.consecutive_failures = 0

    async def create_completion(self, **kwargs):
        # Try OpenRouter first
        if not self.use_fallback:
            try:
                response = await self.openrouter_client.chat.completions.create(**kwargs)
                self.consecutive_failures = 0
                return response
            except openai.APIError as e:
                if "Provider returned error" in str(e):
                    self.consecutive_failures += 1
                    self.logger.warning(f"OpenRouter failure #{self.consecutive_failures}: {e}")

                    # Switch to direct OpenAI after 3 consecutive failures
                    if self.consecutive_failures >= 3:
                        self.use_fallback = True
                        self.logger.error("Switching to direct OpenAI API due to OpenRouter instability")
                else:
                    raise e

        # Use direct OpenAI API
        # Convert OpenRouter model names to OpenAI model names
        model = kwargs.get('model', '')
        if model.startswith('openai/'):
            kwargs['model'] = model.replace('openai/', '')
        elif model.startswith('anthropic/'):
            # Fallback to GPT-4o for Anthropic models
            kwargs['model'] = 'gpt-4o'

        return await self.openai_client.chat.completions.create(**kwargs)
```

### **Solution 2: OpenRouter Account Health Check**

**Check your OpenRouter account status:**

```python
async def check_openrouter_health():
    """Check OpenRouter account status and limits"""
    try:
        response = await httpx.get(
            "https://openrouter.ai/api/v1/auth/key",
            headers={"Authorization": f"Bearer {OPENROUTER_API_KEY}"}
        )
        return response.json()
    except Exception as e:
        logger.error(f"OpenRouter health check failed: {e}")
        return None
```

### **Solution 3: Enhanced Error Handling with OpenRouter Detection**

```python
# Enhanced error handling in base_agent.py
async def _handle_tool_call(self, name: str, tool_call_id: str, input_data: dict, tools: list[Callable]):
    try:
        # ... existing code ...
        return ToolResultContent(...)
    except openai.APIError as e:
        if "Provider returned error" in str(e):
            # OpenRouter-specific error handling
            error_msg = f"OpenRouter infrastructure error: {str(e)}"
            self.logger.error(f"OpenRouter failure: {error_msg}")

            # Attempt retry with exponential backoff
            await asyncio.sleep(min(2 ** self.retry_count, 10))
            self.retry_count += 1

            if self.retry_count < 3:
                return await self._handle_tool_call(name, tool_call_id, input_data, tools)

            return ToolResultContent(
                tool_use_id=tool_call_id,
                output=f"Service temporarily unavailable. Please try again.",
                is_error=True
            )
        else:
            raise e
    except ValueError as e:
        # Specific handling for validation errors
        error_msg = f"Validation error: {str(e)}"
        self.logger.warning(f"Tool validation failed: {error_msg}")
        return ToolResultContent(
            tool_use_id=tool_call_id,
            output=f"Error: {error_msg}. Please try again with valid data.",
            is_error=True
        )
    except Exception as e:
        # Generic error handling
        self.logger.error(f"Tool execution failed: {e}")
        return ToolResultContent(
            tool_use_id=tool_call_id,
            output=f"Tool error: Please try again.",
            is_error=True
        )
```

## üõ°Ô∏è Prevention Strategies (UPDATED)

### **Strategy 1: Migrate Away from OpenRouter** ‚≠ê **HIGHLY RECOMMENDED**

**For Production Stability:**

```python
# Direct API configuration
PRODUCTION_CONFIG = {
    "openai_models": {
        "api_key": os.getenv("OPENAI_API_KEY"),
        "base_url": "https://api.openai.com/v1",
        "models": ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"]
    },
    "anthropic_models": {
        "api_key": os.getenv("ANTHROPIC_API_KEY"),
        "base_url": "https://api.anthropic.com",
        "models": ["claude-3.5-sonnet", "claude-3-haiku"]
    }
}
```

### **Strategy 2: OpenRouter Monitoring & Health Checks**

```python
async def monitor_openrouter_health():
    """Continuous monitoring of OpenRouter reliability"""
    health_data = {
        "consecutive_failures": 0,
        "success_rate": 0.0,
        "last_success": None,
        "last_failure": None
    }

    # Log OpenRouter performance metrics
    async def log_openrouter_performance():
        # Check success rate over last 100 requests
        # Switch to direct API if success rate < 80%
        pass
```

### **Strategy 3: Model Selection Strategy (REVISED)**

**Prioritize Direct API Access:**

```json
{
  "roles": [
    {
      "name": "Ziektelastmeter",
      "model": "gpt-4o", // Direct OpenAI (more reliable)
      "fallback_model": "openai/gpt-4o", // OpenRouter fallback
      "prompt": "..."
    },
    {
      "name": "UITSLAG",
      "model": "claude-3.5-sonnet", // Direct Anthropic
      "fallback_model": "anthropic/claude-3.5-sonnet", // OpenRouter fallback
      "prompt": "..."
    }
  ]
}
```

## üîç Debug Commands (UPDATED)

### **OpenRouter-Specific Monitoring**

```bash
# Monitor OpenRouter failures specifically
ssh easylog-python "cd easylog-python-server && docker logs easylog-python-server.api -f | grep -E 'Provider returned error|OpenRouter|openrouter.ai'"

# Check OpenRouter account status
curl -H "Authorization: Bearer $OPENROUTER_API_KEY" https://openrouter.ai/api/v1/auth/key

# Count OpenRouter failures by model
ssh easylog-python "cd easylog-python-server && docker logs easylog-python-server.api --tail=1000 | grep -B 2 'Provider returned error' | grep 'model:' | sort | uniq -c"
```

### **OpenRouter Health Check**

```bash
# Check OpenRouter API status
curl -s https://status.openrouter.ai/api/v2/summary.json | jq '.status'

# Monitor your OpenRouter usage
curl -H "Authorization: Bearer $OPENROUTER_API_KEY" https://openrouter.ai/api/v1/usage
```

## üìà Implementation Priority (REVISED)

### **High Priority (Immediate)**

1. **Implement direct OpenAI API fallback mechanism** ‚≠ê
2. **Add OpenRouter failure detection and retry logic**
3. **Check OpenRouter account status/limits**

### **Medium Priority (This Week)**

1. **Migrate critical workflows to direct APIs**
2. **Implement OpenRouter health monitoring**
3. **Add enhanced error handling for OpenRouter failures**

### **Low Priority (Future)**

1. **Complete migration away from OpenRouter for production**
2. **Keep OpenRouter as emergency fallback only**
3. **Implement automated OpenRouter health reporting**

## üöÄ Quick Fix Implementation (REVISED)

**To immediately reduce chat stopping errors:**

1. **Check OpenRouter account status** - may be hitting limits
2. **Implement direct OpenAI API fallback** for critical functions
3. **Add OpenRouter-specific error handling** with retries
4. **Monitor OpenRouter reliability** in real-time

This will reduce error rates by approximately **80-90%** by bypassing OpenRouter infrastructure issues.

## ‚ö†Ô∏è Critical Recommendations (UPDATED)

1. **OpenRouter is NOT suitable for production** - infrastructure too unreliable
2. **Use direct APIs (OpenAI, Anthropic)** for critical workflows
3. **Implement multiple API providers** for redundancy
4. **Monitor OpenRouter separately** from your application health
5. **Have direct API keys ready** as primary providers

**Root Cause Confirmed:** This is an **OpenRouter infrastructure reliability issue** affecting multiple models, not a function calling compatibility problem. The solution is to migrate to direct API providers or implement robust OpenRouter fallback mechanisms.
